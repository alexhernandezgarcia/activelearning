{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:\n",
      "  grid_size: 10\n",
      "  normalize_scores: true\n",
      "  train_fraction: 1.0\n",
      "  batch_size: 16384\n",
      "  shuffle: true\n",
      "  train_path: ~/activelearning/my_package/storage/hartmann/data_train.csv\n",
      "  test_path: null\n",
      "  _target_: activelearning.dataset.grid.HartmannDatasetHandler\n",
      "oracle:\n",
      "  _target_: activelearning.oracle.oracle.Hartmann\n",
      "  fidelity: 1\n",
      "  do_domain_map: true\n",
      "selector:\n",
      "  _target_: activelearning.filter.filter.ScoreFilter\n",
      "sampler:\n",
      "  _target_: activelearning.sampler.sampler.RandomSampler\n",
      "  conf:\n",
      "    agent:\n",
      "      random_action_prob: 0.001\n",
      "      optimizer:\n",
      "        lr: 0.0005\n",
      "        n_train_steps: 5000\n",
      "    logger:\n",
      "      do:\n",
      "        online: true\n",
      "      project_name: test_hartmann_gflownet\n",
      "      run_name: identity_lr5e-4 newdata\n",
      "    env:\n",
      "      length: 10\n",
      "      n_dim: 6\n",
      "      cell_min: 0\n",
      "      cell_max: 0.99\n",
      "      reward_func: power\n",
      "      reward_min: 1.0e-08\n",
      "      reward_beta: 1.0\n",
      "      reward_norm: 1.0\n",
      "surrogate:\n",
      "  _target_: activelearning.surrogate.gp_surrogate.SingleTaskGPRegressor\n",
      "acquisition:\n",
      "  _target_: activelearning.acquisition.acquisition.BOTorchAcqHandler\n",
      "  acq_fn_class: qLowerBoundMaxValueEntropy\n",
      "user:\n",
      "  logdir:\n",
      "    root: ./logs\n",
      "  data:\n",
      "    root: ./data\n",
      "device: cpu\n",
      "float_precision: 32\n",
      "budget: 10\n",
      "n_samples: 5\n",
      "seed: 31415\n",
      "maximize: false\n",
      "\n",
      "{'dataset': {'grid_size': 10, 'normalize_scores': True, 'train_fraction': 1.0, 'batch_size': 16384, 'shuffle': True, 'train_path': '~/activelearning/my_package/storage/hartmann/data_train.csv', 'test_path': None, '_target_': 'activelearning.dataset.grid.HartmannDatasetHandler'}, 'oracle': {'_target_': 'activelearning.oracle.oracle.Hartmann', 'fidelity': 1, 'do_domain_map': True}, 'selector': {'_target_': 'activelearning.filter.filter.ScoreFilter'}, 'sampler': {'_target_': 'activelearning.sampler.sampler.RandomSampler', 'conf': {'agent': {'random_action_prob': 0.001, 'optimizer': {'lr': 0.0005, 'n_train_steps': 5000}}, 'logger': {'do': {'online': True}, 'project_name': 'test_hartmann_gflownet', 'run_name': 'identity_lr5e-4 newdata'}, 'env': {'length': 10, 'n_dim': 6, 'cell_min': 0, 'cell_max': 0.99, 'reward_func': 'power', 'reward_min': 1e-08, 'reward_beta': 1.0, 'reward_norm': 1.0}}}, 'surrogate': {'_target_': 'activelearning.surrogate.gp_surrogate.SingleTaskGPRegressor'}, 'acquisition': {'_target_': 'activelearning.acquisition.acquisition.BOTorchAcqHandler', 'acq_fn_class': 'qLowerBoundMaxValueEntropy'}, 'user': {'logdir': {'root': './logs'}, 'data': {'root': './data'}}, 'device': 'cpu', 'float_precision': 32, 'budget': 10, 'n_samples': 5, 'seed': 31415, 'maximize': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/c/christina.humer/.conda/envs/al_new/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'test_hartmann.yaml': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Load Hydra config in notebooks\n",
    "# https://github.com/facebookresearch/hydra/blob/main/examples/jupyter_notebooks/compose_configs_in_notebook.ipynb\n",
    "import os\n",
    "from hydra import initialize_config_dir, compose\n",
    "import hydra\n",
    "from omegaconf import OmegaConf\n",
    "abs_config_dir = os.path.abspath(\"config/\")\n",
    "\n",
    "with initialize_config_dir(version_base=None, config_dir=abs_config_dir):\n",
    "    config = compose(config_name=\"test_hartmann.yaml\", overrides=[])\n",
    "    print(OmegaConf.to_yaml(config))\n",
    "    print(config)\n",
    "\n",
    "config.sampler.conf.logger.do.online = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = config.device\n",
    "n_iterations = config.budget  # TODO: replace with budget\n",
    "grid_size = config.dataset.grid_size\n",
    "n_samples = config.n_samples\n",
    "maximize = config.maximize\n",
    "\n",
    "from gflownet.utils.common import set_float_precision\n",
    "float_prec = config.float_precision\n",
    "# float_prec = set_float_precision(config.float_precision)\n",
    "\n",
    "import matplotlib.colors as cm\n",
    "import matplotlib.pyplot as plt\n",
    "# colors = [\"red\", \"blue\", \"green\", \"orange\", \"brown\", \"pink\"]\n",
    "colors = plt.get_cmap(\"Reds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from activelearning.dataset.grid import HartmannDatasetHandler\n",
    "from activelearning.utils.logger import WandBLogger\n",
    "from activelearning.utils.plotter import ProjectionPlotHelper\n",
    "\n",
    "\n",
    "# Dataset\n",
    "dataset_handler = HartmannDatasetHandler(\n",
    "    grid_size=grid_size,\n",
    "    train_path=\"./data/hartmann/data_train.csv\",\n",
    "    train_fraction=1.0,\n",
    "    float_precision=float_prec,\n",
    ")\n",
    "candidate_set, xi, yi = dataset_handler.get_candidate_set(step=1, as_dataloader=True)\n",
    "plot_set, _, _ = dataset_handler.get_candidate_set(step=2, as_dataloader=False)\n",
    "\n",
    "# logger = WandBLogger(project_name=\"test_hartmann_plots\", run_name=\"RandomSampler RandomFilter\")\n",
    "logger = None\n",
    "\n",
    "# plotter = None\n",
    "# plotter = ProjectionPlotHelper(plot_set[:], logger)\n",
    "plotter = ProjectionPlotHelper(plot_set[:])\n",
    "# plotter.logger = logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/c/christina.humer/.conda/envs/al_new/lib/python3.10/site-packages/botorch/models/gp_regression.py:161: UserWarning: The model inputs are of type torch.float32. It is strongly recommended to use double precision in BoTorch, as this improves both precision and stability and can help avoid numerical errors. See https://github.com/pytorch/botorch/discussions/1444\n",
      "  self._validate_tensor_args(X=transformed_X, Y=train_Y, Yvar=train_Yvar)\n",
      "/home/mila/c/christina.humer/.conda/envs/al_new/lib/python3.10/site-packages/botorch/models/gp_regression.py:161: UserWarning: The model inputs are of type torch.float32. It is strongly recommended to use double precision in BoTorch, as this improves both precision and stability and can help avoid numerical errors. See https://github.com/pytorch/botorch/discussions/1444\n",
      "  self._validate_tensor_args(X=transformed_X, Y=train_Y, Yvar=train_Yvar)\n",
      "/home/mila/c/christina.humer/.conda/envs/al_new/lib/python3.10/site-packages/botorch/models/gp_regression.py:161: UserWarning: The model inputs are of type torch.float32. It is strongly recommended to use double precision in BoTorch, as this improves both precision and stability and can help avoid numerical errors. See https://github.com/pytorch/botorch/discussions/1444\n",
      "  self._validate_tensor_args(X=transformed_X, Y=train_Y, Yvar=train_Yvar)\n",
      "/home/mila/c/christina.humer/.conda/envs/al_new/lib/python3.10/site-packages/botorch/models/gp_regression.py:161: UserWarning: The model inputs are of type torch.float32. It is strongly recommended to use double precision in BoTorch, as this improves both precision and stability and can help avoid numerical errors. See https://github.com/pytorch/botorch/discussions/1444\n",
      "  self._validate_tensor_args(X=transformed_X, Y=train_Y, Yvar=train_Yvar)\n"
     ]
    }
   ],
   "source": [
    "from activelearning.surrogate.gp_surrogate import SingleTaskGPRegressor\n",
    "from activelearning.acquisition.acquisition import BOTorchAcqHandler\n",
    "from activelearning.sampler.sampler import GreedySampler, RandomSampler\n",
    "from activelearning.selector.selector import Selector, ScoreSelector\n",
    "from activelearning.oracle.oracle import HartmannOracle\n",
    "\n",
    "# Oracle\n",
    "oracle = HartmannOracle(fidelity=1, device=device, float_precision=float_prec)\n",
    "\n",
    "if plotter is not None:\n",
    "    fig_oracle, ax_oracle = plotter.plot_function(oracle)\n",
    "\n",
    "\n",
    "best_scores = []\n",
    "\n",
    "for i in range(n_iterations):\n",
    "\n",
    "    train_data, test_data = dataset_handler.get_dataloader()\n",
    "    # print(\"iteration\", i)\n",
    "    # Surrogate (e.g., Bayesian Optimization)\n",
    "    # starts with a clean slate each iteration\n",
    "    surrogate = SingleTaskGPRegressor(\n",
    "        device=device,\n",
    "        float_precision=float_prec,\n",
    "    )\n",
    "    surrogate.fit(train_data)\n",
    "\n",
    "    acq_fn = BOTorchAcqHandler(surrogate.model, \"qLowerBoundMaxValueEntropy\", device, float_prec)\n",
    "\n",
    "    # Sampler (e.g., GFlowNet, or Random Sampler)\n",
    "    # also starts with a clean slate; TODO: experiment with NOT training from scratch\n",
    "    sampler = RandomSampler(\n",
    "        acq_fn,\n",
    "    )\n",
    "    # sampler = GreedySampler(\n",
    "    #     acquisition=acq_fn,\n",
    "    #     device=device,\n",
    "    #     float_precision=float_prec,\n",
    "    # )\n",
    "    # sampler = hydra.utils.instantiate(\n",
    "    #     config.sampler,\n",
    "    #     acquisition=acq_fn,\n",
    "    #     device=device,\n",
    "    #     float_precision=float_prec,\n",
    "    #     _recursive_=False,\n",
    "    # )\n",
    "\n",
    "    sampler.fit()  # only necessary for samplers that train a model\n",
    "\n",
    "    samples = sampler.get_samples(n_samples * 5, candidate_set=candidate_set)\n",
    "\n",
    "    if plotter is not None and hasattr(sampler, \"sampler\"):\n",
    "\n",
    "        def reward_fn(samples):\n",
    "            return sampler.sampler.env.proxy2reward(sampler.sampler.env.proxy(samples))\n",
    "\n",
    "        fig_reward, ax_reward = plotter.plot_function(reward_fn)\n",
    "        fig_reward, ax_reward = plotter.plot_samples(samples, ax_reward, fig_reward)\n",
    "        ax_reward.set_title(\"reward fn + proposed samples of iteration %i\" % i)\n",
    "        plotter.log_figure(fig_reward, \"reward\")\n",
    "\n",
    "    # Selector\n",
    "    # selector = Selector()\n",
    "    selector = ScoreSelector(\n",
    "        acq_fn.get_acquisition_values,\n",
    "        device=device,\n",
    "        float_precision=float_prec,\n",
    "    )\n",
    "    filtered_samples = selector(n_samples=n_samples, candidate_set=samples.clone())\n",
    "\n",
    "    if plotter is not None:\n",
    "        fig_acq, ax_acq = plotter.plot_function(acq_fn)\n",
    "        fig_acq, ax_acq = plotter.plot_samples(filtered_samples, ax_acq, fig_acq)\n",
    "        ax_acq.set_title(\"acquisition fn + selected samples of iteration %i\" % i)\n",
    "        plotter.log_figure(fig_acq, \"acq\")\n",
    "\n",
    "    if plotter is not None:\n",
    "        fig_acq, ax_acq = plotter.plot_samples(\n",
    "            filtered_samples,\n",
    "            ax_oracle,\n",
    "            fig_oracle,\n",
    "            c=cm.to_hex(colors(i / n_iterations)),\n",
    "            label=\"it %i\" % i,\n",
    "        )\n",
    "\n",
    "    del surrogate\n",
    "    del sampler\n",
    "    del selector\n",
    "\n",
    "    scores = oracle(filtered_samples.clone())\n",
    "    dataset_handler.update_dataset(filtered_samples.cpu(), scores.cpu())\n",
    "    best_scores.append(scores.min().cpu())\n",
    "    if logger is not None:\n",
    "        logger.log_metric(scores.min().cpu(), \"best_score\")\n",
    "\n",
    "if plotter is not None:\n",
    "    fig_oracle.legend()\n",
    "    ax_oracle.set_title(\"oracle fn + samples\")\n",
    "    plotter.log_figure(fig_oracle, key=\"oracle\")\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(best_scores)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"scores\")\n",
    "plt.title(\"Best Score in each iteration\")\n",
    "if plotter is not None:\n",
    "    plotter.log_figure(fig, key=\"best_scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

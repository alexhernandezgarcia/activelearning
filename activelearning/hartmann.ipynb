{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:\n",
      "  grid_size: 10\n",
      "  normalize_scores: true\n",
      "  train_fraction: 1.0\n",
      "  batch_size: 16384\n",
      "  shuffle: true\n",
      "  train_path: ~/activelearning/my_package/storage/hartmann/data_train.csv\n",
      "  test_path: null\n",
      "  _target_: dataset.grid.HartmannDatasetHandler\n",
      "oracle:\n",
      "  _target_: oracle.oracle.Hartmann\n",
      "  fidelity: 1\n",
      "  do_domain_map: true\n",
      "filter:\n",
      "  _target_: filter.filter.ScoreFilter\n",
      "sampler:\n",
      "  conf:\n",
      "    state_flow: null\n",
      "    policy:\n",
      "      forward:\n",
      "        _target_: gflownet.policy.base.Policy\n",
      "        config:\n",
      "          type: mlp\n",
      "          n_hid: 2048\n",
      "          n_layers: 2\n",
      "          checkpoint: null\n",
      "          reload_ckpt: false\n",
      "          is_model: false\n",
      "      backward:\n",
      "        _target_: gflownet.policy.base.Policy\n",
      "        config: null\n",
      "      shared: null\n",
      "    env:\n",
      "      _target_: gflownet.envs.grid.Grid\n",
      "      id: grid\n",
      "      func: corners\n",
      "      n_dim: 6\n",
      "      length: 10\n",
      "      max_increment: 1\n",
      "      max_dim_per_action: 1\n",
      "      cell_min: 0\n",
      "      cell_max: 0.99\n",
      "      buffer:\n",
      "        train: null\n",
      "        test: null\n",
      "      reward_func: power\n",
      "      reward_min: 1.0e-08\n",
      "      reward_beta: 1.0\n",
      "      reward_norm: 1.0\n",
      "      reward_norm_std_mult: 0.0\n",
      "      proxy_state_format: oracle\n",
      "      skip_mask_check: false\n",
      "      conditional: false\n",
      "      continuous: false\n",
      "    agent:\n",
      "      _target_: gflownet.gflownet.GFlowNetAgent\n",
      "      seed: 0\n",
      "      optimizer:\n",
      "        z_dim: 16\n",
      "        loss: trajectorybalance\n",
      "        lr: 0.0005\n",
      "        lr_decay_period: 1000000\n",
      "        lr_decay_gamma: 0.5\n",
      "        lr_z_mult: 20\n",
      "        method: adam\n",
      "        early_stopping: 0.0\n",
      "        ema_alpha: 0.5\n",
      "        adam_beta1: 0.9\n",
      "        adam_beta2: 0.999\n",
      "        sgd_momentum: 0.9\n",
      "        batch_size:\n",
      "          forward: 16\n",
      "          backward_dataset: 0\n",
      "          backward_replay: 0\n",
      "        train_to_sample_ratio: 1\n",
      "        n_train_steps: 5000\n",
      "        bootstrap_tau: 0.0\n",
      "        clip_grad_norm: 0.0\n",
      "      batch_reward: true\n",
      "      mask_invalid_actions: true\n",
      "      temperature_logits: 1.0\n",
      "      random_action_prob: 0.001\n",
      "      pct_offline: 0.0\n",
      "      replay_capacity: 0\n",
      "      replay_sampling: permutation\n",
      "      train_sampling: permutation\n",
      "      num_empirical_loss: 200000\n",
      "      oracle:\n",
      "        'n': 50\n",
      "      sample_only: false\n",
      "      active_learning: false\n",
      "      buffer:\n",
      "        train: null\n",
      "        test: null\n",
      "    logger:\n",
      "      _target_: gflownet.utils.logger.Logger\n",
      "      do:\n",
      "        online: true\n",
      "        times: false\n",
      "      project_name: test_hartmann_gflownet\n",
      "      train:\n",
      "        period: 1\n",
      "      test:\n",
      "        first_it: true\n",
      "        period: 1000\n",
      "        'n': 100\n",
      "        kde:\n",
      "          bandwidth: 0.1\n",
      "          kernel: gaussian\n",
      "        n_top_k: 5000\n",
      "        top_k: 100\n",
      "        top_k_period: -1\n",
      "        n_trajs_logprobs: 10\n",
      "        logprobs_batch_size: 100\n",
      "        logprobs_bootstrap_size: 10000\n",
      "        max_data_logprobs: 100000.0\n",
      "      oracle:\n",
      "        period: 100000\n",
      "        k:\n",
      "        - 1\n",
      "        - 10\n",
      "        - 100\n",
      "      checkpoints:\n",
      "        period: 1000\n",
      "      logdir:\n",
      "        root: activelearning/gflownet/logs\n",
      "        ckpts: activelearning/gflownetckpts\n",
      "        overwrite: true\n",
      "      debug: false\n",
      "      lightweight: false\n",
      "      progress: true\n",
      "      context: '0'\n",
      "      notes: null\n",
      "      tags:\n",
      "      - gflownet\n",
      "      run_name: identity_lr5e-4 newdata\n",
      "  _target_: sampler.sampler.GFlowNetSampler\n",
      "surrogate:\n",
      "  _target_: surrogate.surrogate.SingleTaskGPRegressor\n",
      "user:\n",
      "  logdir:\n",
      "    root: activelearning/logs\n",
      "  data:\n",
      "    root: activelearning/data\n",
      "device: cpu\n",
      "float_precision: 32\n",
      "budget: 10\n",
      "n_samples: 5\n",
      "seed: 31415\n",
      "maximize: false\n",
      "\n",
      "{'dataset': {'grid_size': 10, 'normalize_scores': True, 'train_fraction': 1.0, 'batch_size': 16384, 'shuffle': True, 'train_path': '~/activelearning/my_package/storage/hartmann/data_train.csv', 'test_path': None, '_target_': 'dataset.grid.HartmannDatasetHandler'}, 'oracle': {'_target_': 'oracle.oracle.Hartmann', 'fidelity': 1, 'do_domain_map': True}, 'filter': {'_target_': 'filter.filter.ScoreFilter'}, 'sampler': {'conf': {'state_flow': None, 'policy': {'forward': {'_target_': 'gflownet.policy.base.Policy', 'config': {'type': 'mlp', 'n_hid': 2048, 'n_layers': 2, 'checkpoint': None, 'reload_ckpt': False, 'is_model': False}}, 'backward': {'_target_': 'gflownet.policy.base.Policy', 'config': None}, 'shared': None}, 'env': {'_target_': 'gflownet.envs.grid.Grid', 'id': 'grid', 'func': 'corners', 'n_dim': 6, 'length': 10, 'max_increment': 1, 'max_dim_per_action': 1, 'cell_min': 0, 'cell_max': 0.99, 'buffer': {'train': None, 'test': None}, 'reward_func': 'power', 'reward_min': 1e-08, 'reward_beta': 1.0, 'reward_norm': 1.0, 'reward_norm_std_mult': 0.0, 'proxy_state_format': 'oracle', 'skip_mask_check': False, 'conditional': False, 'continuous': False}, 'agent': {'_target_': 'gflownet.gflownet.GFlowNetAgent', 'seed': 0, 'optimizer': {'z_dim': 16, 'loss': 'trajectorybalance', 'lr': 0.0005, 'lr_decay_period': 1000000, 'lr_decay_gamma': 0.5, 'lr_z_mult': 20, 'method': 'adam', 'early_stopping': 0.0, 'ema_alpha': 0.5, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'sgd_momentum': 0.9, 'batch_size': {'forward': 16, 'backward_dataset': 0, 'backward_replay': 0}, 'train_to_sample_ratio': 1, 'n_train_steps': 5000, 'bootstrap_tau': 0.0, 'clip_grad_norm': 0.0}, 'batch_reward': True, 'mask_invalid_actions': True, 'temperature_logits': 1.0, 'random_action_prob': 0.001, 'pct_offline': 0.0, 'replay_capacity': 0, 'replay_sampling': 'permutation', 'train_sampling': 'permutation', 'num_empirical_loss': 200000, 'oracle': {'n': 50}, 'sample_only': False, 'active_learning': False, 'buffer': {'train': None, 'test': None}}, 'logger': {'_target_': 'gflownet.utils.logger.Logger', 'do': {'online': True, 'times': False}, 'project_name': 'test_hartmann_gflownet', 'train': {'period': 1}, 'test': {'first_it': True, 'period': 1000, 'n': 100, 'kde': {'bandwidth': 0.1, 'kernel': 'gaussian'}, 'n_top_k': 5000, 'top_k': 100, 'top_k_period': -1, 'n_trajs_logprobs': 10, 'logprobs_batch_size': 100, 'logprobs_bootstrap_size': 10000, 'max_data_logprobs': 100000.0}, 'oracle': {'period': 100000, 'k': [1, 10, 100]}, 'checkpoints': {'period': 1000}, 'logdir': {'root': 'activelearning/gflownet/logs', 'ckpts': 'activelearning/gflownetckpts', 'overwrite': True}, 'debug': False, 'lightweight': False, 'progress': True, 'context': '0', 'notes': None, 'tags': ['gflownet'], 'run_name': 'identity_lr5e-4 newdata'}}, '_target_': 'sampler.sampler.GFlowNetSampler'}, 'surrogate': {'_target_': 'surrogate.surrogate.SingleTaskGPRegressor'}, 'user': {'logdir': {'root': 'activelearning/logs'}, 'data': {'root': 'activelearning/data'}}, 'device': 'cpu', 'float_precision': 32, 'budget': 10, 'n_samples': 5, 'seed': 31415, 'maximize': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/c/christina.humer/.conda/envs/al_new/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'test_hartmann.yaml': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Load Hydra config in notebooks\n",
    "# https://github.com/facebookresearch/hydra/blob/main/examples/jupyter_notebooks/compose_configs_in_notebook.ipynb\n",
    "import os\n",
    "from hydra import initialize_config_dir, compose\n",
    "import hydra\n",
    "from omegaconf import OmegaConf\n",
    "abs_config_dir = os.path.abspath(\"config/\")\n",
    "\n",
    "with initialize_config_dir(version_base=None, config_dir=abs_config_dir):\n",
    "    config = compose(config_name=\"test_hartmann.yaml\", overrides=[])\n",
    "    print(OmegaConf.to_yaml(config))\n",
    "    print(config)\n",
    "\n",
    "config.sampler.conf.logger.do.online = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = config.device\n",
    "n_iterations = config.budget  # TODO: replace with budget\n",
    "grid_size = config.dataset.grid_size\n",
    "n_samples = config.n_samples\n",
    "maximize = config.maximize\n",
    "\n",
    "from gflownet.utils.common import set_float_precision\n",
    "float_prec = set_float_precision(config.float_precision)\n",
    "\n",
    "import matplotlib.colors as cm\n",
    "import matplotlib.pyplot as plt\n",
    "# colors = [\"red\", \"blue\", \"green\", \"orange\", \"brown\", \"pink\"]\n",
    "colors = plt.get_cmap(\"Reds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchristina-humer\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mila/c/christina.humer/activelearning/my_package/wandb/run-20240404_040345-uz82iubu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/christina-humer/test_hartmann_plots/runs/uz82iubu' target=\"_blank\">RandomSampler RandomFilter</a></strong> to <a href='https://wandb.ai/christina-humer/test_hartmann_plots' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/christina-humer/test_hartmann_plots' target=\"_blank\">https://wandb.ai/christina-humer/test_hartmann_plots</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/christina-humer/test_hartmann_plots/runs/uz82iubu' target=\"_blank\">https://wandb.ai/christina-humer/test_hartmann_plots/runs/uz82iubu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dataset.grid import HartmannDatasetHandler\n",
    "from utils.logger import WandBLogger\n",
    "from utils.plotter import ProjectionPlotHelper\n",
    "\n",
    "\n",
    "# Dataset\n",
    "dataset_handler = HartmannDatasetHandler(\n",
    "    grid_size=grid_size,\n",
    "    train_path=\"./storage/hartmann/data_train.csv\",\n",
    "    train_fraction=1.0,\n",
    "    float_precision=float_prec,\n",
    ")\n",
    "candidate_set, xi, yi = dataset_handler.get_candidate_set(step=1, as_dataloader=True)\n",
    "plot_set, _, _ = dataset_handler.get_candidate_set(step=2, as_dataloader=False)\n",
    "\n",
    "logger = WandBLogger(project_name=\"test_hartmann_plots\", run_name=\"RandomSampler RandomFilter\")\n",
    "# plotter = None\n",
    "plotter = ProjectionPlotHelper(plot_set[:], logger)\n",
    "# plotter.logger = logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/c/christina.humer/.conda/envs/al_new/lib/python3.10/site-packages/botorch/models/gp_regression.py:161: UserWarning: The model inputs are of type torch.float32. It is strongly recommended to use double precision in BoTorch, as this improves both precision and stability and can help avoid numerical errors. See https://github.com/pytorch/botorch/discussions/1444\n",
      "  self._validate_tensor_args(X=transformed_X, Y=train_Y, Yvar=train_Yvar)\n",
      "/home/mila/c/christina.humer/.conda/envs/al_new/lib/python3.10/site-packages/botorch/models/gp_regression.py:161: UserWarning: The model inputs are of type torch.float32. It is strongly recommended to use double precision in BoTorch, as this improves both precision and stability and can help avoid numerical errors. See https://github.com/pytorch/botorch/discussions/1444\n",
      "  self._validate_tensor_args(X=transformed_X, Y=train_Y, Yvar=train_Yvar)\n",
      "/home/mila/c/christina.humer/.conda/envs/al_new/lib/python3.10/site-packages/botorch/models/gp_regression.py:161: UserWarning: The model inputs are of type torch.float32. It is strongly recommended to use double precision in BoTorch, as this improves both precision and stability and can help avoid numerical errors. See https://github.com/pytorch/botorch/discussions/1444\n",
      "  self._validate_tensor_args(X=transformed_X, Y=train_Y, Yvar=train_Yvar)\n",
      "/home/mila/c/christina.humer/.conda/envs/al_new/lib/python3.10/site-packages/botorch/models/gp_regression.py:161: UserWarning: The model inputs are of type torch.float32. It is strongly recommended to use double precision in BoTorch, as this improves both precision and stability and can help avoid numerical errors. See https://github.com/pytorch/botorch/discussions/1444\n",
      "  self._validate_tensor_args(X=transformed_X, Y=train_Y, Yvar=train_Yvar)\n",
      "/home/mila/c/christina.humer/.conda/envs/al_new/lib/python3.10/site-packages/botorch/models/gp_regression.py:161: UserWarning: The model inputs are of type torch.float32. It is strongly recommended to use double precision in BoTorch, as this improves both precision and stability and can help avoid numerical errors. See https://github.com/pytorch/botorch/discussions/1444\n",
      "  self._validate_tensor_args(X=transformed_X, Y=train_Y, Yvar=train_Yvar)\n",
      "/home/mila/c/christina.humer/.conda/envs/al_new/lib/python3.10/site-packages/botorch/models/gp_regression.py:161: UserWarning: The model inputs are of type torch.float32. It is strongly recommended to use double precision in BoTorch, as this improves both precision and stability and can help avoid numerical errors. See https://github.com/pytorch/botorch/discussions/1444\n",
      "  self._validate_tensor_args(X=transformed_X, Y=train_Y, Yvar=train_Yvar)\n",
      "/home/mila/c/christina.humer/.conda/envs/al_new/lib/python3.10/site-packages/botorch/models/gp_regression.py:161: UserWarning: The model inputs are of type torch.float32. It is strongly recommended to use double precision in BoTorch, as this improves both precision and stability and can help avoid numerical errors. See https://github.com/pytorch/botorch/discussions/1444\n",
      "  self._validate_tensor_args(X=transformed_X, Y=train_Y, Yvar=train_Yvar)\n",
      "/home/mila/c/christina.humer/.conda/envs/al_new/lib/python3.10/site-packages/botorch/models/gp_regression.py:161: UserWarning: The model inputs are of type torch.float32. It is strongly recommended to use double precision in BoTorch, as this improves both precision and stability and can help avoid numerical errors. See https://github.com/pytorch/botorch/discussions/1444\n",
      "  self._validate_tensor_args(X=transformed_X, Y=train_Y, Yvar=train_Yvar)\n",
      "/home/mila/c/christina.humer/.conda/envs/al_new/lib/python3.10/site-packages/botorch/models/gp_regression.py:161: UserWarning: The model inputs are of type torch.float32. It is strongly recommended to use double precision in BoTorch, as this improves both precision and stability and can help avoid numerical errors. See https://github.com/pytorch/botorch/discussions/1444\n",
      "  self._validate_tensor_args(X=transformed_X, Y=train_Y, Yvar=train_Yvar)\n",
      "/home/mila/c/christina.humer/.conda/envs/al_new/lib/python3.10/site-packages/botorch/models/gp_regression.py:161: UserWarning: The model inputs are of type torch.float32. It is strongly recommended to use double precision in BoTorch, as this improves both precision and stability and can help avoid numerical errors. See https://github.com/pytorch/botorch/discussions/1444\n",
      "  self._validate_tensor_args(X=transformed_X, Y=train_Y, Yvar=train_Yvar)\n"
     ]
    }
   ],
   "source": [
    "from surrogate.surrogate import SingleTaskGPRegressor\n",
    "from sampler.sampler import GreedySampler, RandomSampler\n",
    "from filter.filter import Filter, ScoreFilter\n",
    "from oracle.oracle import HartmannOracle\n",
    "\n",
    "# Oracle\n",
    "oracle = HartmannOracle(fidelity=1, device=device, float_precision=float_prec)\n",
    "\n",
    "if plotter is not None:\n",
    "    fig_oracle, ax_oracle = plotter.plot_function(oracle)\n",
    "\n",
    "\n",
    "best_scores = []\n",
    "\n",
    "for i in range(n_iterations):\n",
    "\n",
    "    train_data, test_data = dataset_handler.get_dataloader()\n",
    "    # print(\"iteration\", i)\n",
    "    # Surrogate (e.g., Bayesian Optimization)\n",
    "    # starts with a clean slate each iteration\n",
    "    surrogate = SingleTaskGPRegressor(\n",
    "        float_precision=float_prec, device=device, maximize=maximize\n",
    "    )\n",
    "    surrogate.fit(train_data)\n",
    "\n",
    "    # Sampler (e.g., GFlowNet, or Random Sampler)\n",
    "    # also starts with a clean slate; TODO: experiment with NOT training from scratch\n",
    "    sampler = RandomSampler(\n",
    "        surrogate,\n",
    "    )\n",
    "    # sampler = GreedySampler(\n",
    "    #     surrogate,\n",
    "    #     device=device,\n",
    "    #     float_precision=float_prec,\n",
    "    # )\n",
    "    # sampler = hydra.utils.instantiate(\n",
    "    #     config.sampler,\n",
    "    #     surrogate=surrogate,\n",
    "    #     device=device,\n",
    "    #     float_precision=float_prec,\n",
    "    #     _recursive_=False,\n",
    "    # )\n",
    "\n",
    "    sampler.fit()  # only necessary for samplers that train a model\n",
    "\n",
    "    samples = sampler.get_samples(n_samples * 5, candidate_set=candidate_set)\n",
    "    \n",
    "    if plotter is not None and hasattr(sampler, \"sampler\"):\n",
    "        def reward_fn(samples):\n",
    "            return sampler.sampler.env.proxy2reward(sampler.sampler.env.proxy(samples))\n",
    "        fig_reward, ax_reward = plotter.plot_function(reward_fn)\n",
    "        fig_reward, ax_reward = plotter.plot_samples(samples, ax_reward, fig_reward)\n",
    "        ax_reward.set_title(\"reward fn + proposed samples of iteration %i\"%i)\n",
    "        plotter.log_figure(fig_reward, \"reward\")\n",
    "\n",
    "\n",
    "    # Filter\n",
    "    # filter = Filter()\n",
    "    filter = ScoreFilter(surrogate.get_acquisition_values)\n",
    "    filtered_samples = filter(n_samples=n_samples, candidate_set=samples.clone())\n",
    "\n",
    "    if plotter is not None:\n",
    "        fig_acq, ax_acq = plotter.plot_function(surrogate)\n",
    "        fig_acq, ax_acq = plotter.plot_samples(filtered_samples, ax_acq, fig_acq)\n",
    "        ax_acq.set_title(\"acquisition fn + selected samples of iteration %i\" % i)\n",
    "        plotter.log_figure(fig_acq, \"acq\")\n",
    "\n",
    "    if plotter is not None:\n",
    "        fig_acq, ax_acq = plotter.plot_samples(\n",
    "            filtered_samples,\n",
    "            ax_oracle,\n",
    "            fig_oracle,\n",
    "            c=cm.to_hex(colors(i / n_iterations)),\n",
    "            label=\"it %i\" % i,\n",
    "        )\n",
    "\n",
    "    del surrogate\n",
    "    del sampler\n",
    "    del filter\n",
    "\n",
    "    scores = oracle(filtered_samples.clone())\n",
    "    dataset_handler.update_dataset(filtered_samples.cpu(), scores.cpu())\n",
    "    best_scores.append(scores.min().cpu())\n",
    "    if logger is not None:\n",
    "        logger.log_metric(scores.min().cpu(), \"best_score\")\n",
    "\n",
    "if plotter is not None:\n",
    "    fig_oracle.legend()\n",
    "    ax_oracle.set_title(\"oracle fn + samples\")\n",
    "    plotter.log_figure(fig_oracle, key=\"oracle\")\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(best_scores)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"scores\")\n",
    "plt.title(\"Best Score in each iteration\")\n",
    "if plotter is not None:\n",
    "    plotter.log_figure(fig, key=\"best_scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

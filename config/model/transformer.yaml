_target_: model.mlp.Transformer

hidden_dim: 256
num_layer: 2
dropout_prob: 0.0
num_output: 1
num_head: 8
pre_ln: True
factor: 2
activation: 'relu'
mlp:
  num_layer: 2
  dropout_prob: 0.1

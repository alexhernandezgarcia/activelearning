_target_: model.mlp.MLP

activation: "relu"
num_output: 1
n_hid: 64
feature_dim: ${model.n_hid}
dropout_prob: 0.1
n_layers: 4

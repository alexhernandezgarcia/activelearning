# @package _global_

defaults:
  # - _self_
  - env: mols
  - oracle@_oracle_dict.1: ip_fid3
  - gflownet: trajectorybalance
  - user: nikita
  - proxy: mes_svgp #botorch_ucb #mes_svgp
  - dataset: dataset
  - regressor: sf_dkl #dropout_regressor
  - model: mlm_transformer #mlm_transformer 
  - logger: wandb

regressor:
  surrogate: 
    num_epochs: 512
    patience: 15
    eval_period: 1

# REMEMBER TO UPDATE TRANSFORMER CONFIG TO 32
logger:
  # resume: True 
  # logdir: 
  #   root: /home/mila/n/nikita.saxena/scratch/logs/activelearning/2023-05-02_16-47-16
  #   overwrite: False
  # run_id: win8pvxt #fuy1i0gz  #uwjujwlg
  project_name: "Scratch" #"Aptamers-DKL"
  lightweight: False
  plot:
    period: -1
    first_it: True
  test:
    period: 1000
    n: 1
    first_it: True
  oracle:
    period: -1
  # ckpts:
  #   policy:
  #     period: 1000
  do:
    times: False
    online: True
  tags:
    - sf

env:
  denorm_proxy: False #True for ucb, False for mes
  corr_type: None #from_trajectory
  proxy_state_format: state
  reward_func: power #boltzmann
  reward_beta: 1 #0.2 for ucb and 2e1 for mes
  norm_factor: 1.5
  # length: 10
  reward_norm: 0.001
  buffer:
    train:
      path: data_train.csv
      # /home/mila/n/nikita.saxena/activelearning/storage/amp/data_train.csv
      n: null
      type: all
      seed: null
      output_pkl: buffer_data_train.csv 
      output_csv: null
    test:
      path: data_test.csv
      type: all
      output_pkl: buffer_data_test.csv
      n: null
      seed: null
      output_csv: null
    

gflownet:
  active_learning: True
  # sample_only: True
  random_action_prob: 0.0 #0.001
  optimizer:
    lr: 5e-4
    lr_z_mult: 20
    n_train_steps: 10000
    batch_size: 32
  policy:
    forward:
      type: mlp
      n_hid: 2048
      n_layers: 2
      checkpoint: fp
    # backward:
    #   type: mlp
    #   shared_weights: True
  pct_offline: 0.0 #0.3
  oracle:
    n: 500

dataset:
  normalize_data: True
  # n_samples: 30
  split: given #random
  train_fraction: 0.8
  dataloader:
    train:
      batch_size: 32
    test:
      batch_size: 32
  path:
    type: sf
    oracle_dataset:
      train: 
        # path: /home/mila/n/nikita.saxena/activelearning/storage/dna/length30/oracle_dataset.csv
        path:  ${user.data_path}/mols/fid3/150_train_IP.csv
        # path: /home/mila/n/nikita.saxena/activelearning/storage/mols/IP/sf/data_train.csv
        # path: /home/mila/n/nikita.saxena/scratch/logs/activelearning/2023-05-02_16-47-16/data/data_train.csv
        # path: /home/mila/n/nikita.saxena/scratch/logs/activelearning/2023-04-25_12-14-24/data/data_sampled_iter1.csv
        # path: /home/mila/n/nikita.saxena/scratch/logs/activelearning/2023-04-25_12-14-24/data/data_train.csv
        # path:  /home/mila/n/nikita.saxena/activelearning/storage/dna/sampled_data_sf.csv
        # path: /home/mila/n/nikita.saxena/activelearning/storage/dna/sampled_init_combined_sf.csv
        # TODO: Need to make this true before actual experiment
        get_scores: False
      test: 
        # path: ${user.data_path}/mols/mols.csv
        path: ${user.data_path}/mols/fid3/150_test_IP.csv
        # path: /home/mila/n/nikita.saxena/activelearning/storage/mols/IP/sf/data_test.csv
        # # ${user.data_path}/dna/length30/exact_dkl/sf/data_test.csv
        # # /home/mila/n/nikita.saxena/activelearning/storage/dna/length30/test_2000.csv
        get_scores: False
# Number of objects to sample at the end of training
# Sample 5*K and choose topK
n_samples: 128
# Random seeds
seed: 0
# Device
device: cuda
# Float precision
float_precision: 32
#It is strongly recommended to use double precision in BoTorch, as this improves both precision and stability and can help avoid numerical errors. See https://github.com/pytorch/botorch/discussions/1444
al_n_rounds: 10
do_figure: False

multifidelity:
  proxy: True
  #  /home/mila/n/nikita.saxena/activelearning/storage/grid/nfid3_lower_quad.csv
  # /home/mila/n/nikita.saxena/activelearning/storage/grid/nfid3.csv
  # proxy_state_format: oracle
  fixed_cost: 0.0
  rescale: 1

# Hydra config
hydra:
  # See: https://hydra.cc/docs/configure_hydra/workdir/
  run:
    dir: ${user.logdir.root}/${now:%Y-%m-%d_%H-%M-%S}
  job:
    # See: https://hydra.cc/docs/upgrades/1.1_to_1.2/changes_to_job_working_dir/
    # See: https://hydra.cc/docs/tutorials/basic/running_your_app/working_directory/#disable-changing-current-working-dir-to-jobs-output-dir
    chdir: True

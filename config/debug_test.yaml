
defaults:
  # - _self_
  - env: grid
  - oracle@_oracle_dict.1: hartmann
  - gflownet: trajectorybalance
  - user: nikita
  - proxy: mes_gp 
  - dataset: dataset
  - regressor: sf_gp 
  # - model: mlp 
  - logger: wandb

# regressor:
#   surrogate:
#     eval_period: 1
#     num_epochs: 2048
#     patience: 100
#     num_inducing_points: 64

# model:
#   feature_dim: 64
#   n_hid: 64

logger:
  project_name: "Scratch"
  lightweight: False
  plot:
    period: -1
    first_it: True
  test:
    period: -1
    n: 1
  oracle:
    period: -1
  do:
    times: False
    online: True
  tags:
    - hartmann

env:
  corr_type: None
  n_dim: 6
  proxy_state_format: state
  reward_func: power
  length: 10
  reward_beta: 1
  rescale: ${env.length}
  reward_norm: 0.00125
  buffer:
    train:
      path: null
      n: null
      type: null
      seed: null
      output_csv: null 
    test:
      path: null 
      type: null
      output_pkl: null 
      n: null 
      seed: null
      output_csv: null 
    

gflownet:
  active_learning: True
  sample_only: True
  random_action_prob: 0.001
  optimizer:
    lr: 5e-4
    lr_z_mult: 20
    n_train_steps: 10000
    batch_size: 32
  policy:
    forward:
      type: mlp
      n_hid: 2048
      n_layers: 2
      checkpoint: fp
    backward:
      type: mlp
      shared_weights: True
  pct_offline: 0.0
  oracle:
    n: 500

dataset:
  normalize_data: True
  n_samples: 300
  split: random
  train_fraction: 0.9
  dataloader:
    train:
      batch_size: 64
    test:
      batch_size: 64
  path:
    type: sf
    oracle_dataset: 
      train:
        path: ${user.data_path}/hartmann/sf/data_train.csv
        get_scores: True
      test: null
        # path: ${user.data_path}/hartmann/sf/data_test.csv
        # get_scores: True
# Number of objects to sample at the end of training
# Sample 5*K and choose topK
n_samples: 10
# Random seeds
seed: 0
# Device
device: cuda
# Float precision
float_precision: 64
al_n_rounds: 10
do_figure: False

multifidelity:
  proxy: True
  fid_embed: one_hot
  fid_embed_dim: None

# Hydra config
hydra:
  # See: https://hydra.cc/docs/configure_hydra/workdir/
  run:
    dir: ${user.logdir.root}/${now:%Y-%m-%d_%H-%M-%S}
  job:
    # See: https://hydra.cc/docs/upgrades/1.1_to_1.2/changes_to_job_working_dir/
    # See: https://hydra.cc/docs/tutorials/basic/running_your_app/working_directory/#disable-changing-current-working-dir-to-jobs-output-dir
    chdir: True


# # @package _global_

# defaults:
#   # - _self_
#   - env: amp
#   - oracle@_oracle_dict.1: amp
#   # - oracle: amp
#   - gflownet: trajectorybalance
#   - user: nikita
#   - proxy: dropout_regressor
#   - dataset: dataset
#   - regressor: dropout_regressor
#   - model: mlp
#   - logger: wandb

# logger:
#   project_name: "Scratch"
#   test:
#     period: 100
#   oracle:
#     period: 1000
#   do:
#     times: False
#     online: False
#   plot:
#     period: 1000 #1000
#     first_it: True
#   tags:
#     - (10x)^8
#     - amp
#     - train_frac = 0.8
#     - tb
#     - get_scores_realtime
#     # - botorch_ucb
#     # - backward_policy = shared

# env:
#   reward_func: power
#   proxy_state_format: ohe
#   reward_beta: 8.0
#   reward_norm: 0.1
#   buffer:
#     train:
#       path: null
#       n: null
#       seed: null
#       output_csv: null 
#     test:
#       path: data_test.csv
#       # path: data_test.csv
#       type: all
#       output_pkl: buffer_data_test.csv
#       n: null
#       seed: null
#       output_csv: null 
    

# gflownet:
#   active_learning: True
#   sample_only: False
#   random_action_prob: 0.001
#   optimizer:
#     lr: 5e-4
#     lr_z_mult: 20
#     n_train_steps: 10
#     batch_size: 5
#   policy:
#     forward:
#       type: mlp
#       n_hid: 2048
#       n_layers: 2
#       checkpoint: fp
#     backward:
#       type: mlp
#       # shared_weights: True
#   oracle:
#     n: 500

# model:
#   n_hid: 2048
#   n_layers: 2

# regressor:
#   weight_decay: 1e-4
#   lr: 1e-3
#   max_epochs: 3
#   history: 1

# dataset:
#   split: random
#   fidelity: null
#   train_fraction: 0.9
#   dataloader:
#     train:
#       batch_size: 256
#     test:
#       batch_size: 256
#   path:
#     oracle_dataset:
#       train: 
#         path: /home/mila/n/nikita.saxena/activelearning/storage/amp/data_train.csv
#         get_scores: False
#       test: 
#         path: /home/mila/n/nikita.saxena/activelearning/storage/amp/data_test.csv
#         get_scores: False
  
# # Number of objects to sample at the end of training
# # Sample 5*K and choose topK
# n_samples: 2
# # Random seeds
# seed: 0
# # Device
# device: cuda
# # Float precision
# float_precision: 32
# al_n_rounds: 15

# multifidelity:
#   toy: True
#   rescale: 1

# # Hydra config
# hydra:
#   # See: https://hydra.cc/docs/configure_hydra/workdir/
#   run:
#     dir: ${user.logdir.root}/${now:%Y-%m-%d_%H-%M-%S}
#   job:
#     # See: https://hydra.cc/docs/upgrades/1.1_to_1.2/changes_to_job_working_dir/
#     # See: https://hydra.cc/docs/tutorials/basic/running_your_app/working_directory/#disable-changing-current-working-dir-to-jobs-output-dir
#     chdir: True
# @package _global_

defaults:
  # - _self_
  - env: grid
  - true_oracle: corners
  - oracle@_oracle_dict.1: corners_fid3
  - oracle@_oracle_dict.2: corners_fid2
  - oracle@_oracle_dict.3: corners_fid1
  - gflownet: trajectorybalance
  - user: nikita
  - proxy: mf_mes_oracle
  - dataset: dataset
  - regressor: dropout_regressor
  - model: mf_mlp
  - logger: wandb

# proxy:
  # is_model_oracle: True

logger:
  project_name: "Scratch"
  lightweight: False
  plot:
    period: 1000
    first_it: True
  test:
    period: 100
    n: 1000
  oracle:
    period: 1000
  # ckpts:
  #   policy:
  #     period: 1000
  do:
    times: False
    online: True
  tags:
    - corr_test
    - r(x) = 100x
    - 20x20
    - tb
    - nn-mf-mes
    - backward_policy = shared
    - sampling_temp=1.0
    - lower_quad
    - sample_1000_samples
    - random_action=0.1
    # - inc_variation_among_fid

env:
  length: 10
  proxy_state_format: oracle
  reward_func: power
  reward_beta: 1
  reward_norm: 1e-2
  buffer:
    train:
      path: null
      # /home/mila/n/nikita.saxena/activelearning/storage/amp/data_train.csv
      n: null
      seed: null
      output_csv: null 
    test:
      path: null
    # TODO: change path to a larger initial dataset
      type: all
      output_pkl: data_test.csv
      n: null
      seed: null
      output_csv: null 
    
gflownet:
  sample_only: True
  random_action_prob: 0.1
  optimizer:
    lr: 5e-4
    lr_z_mult: 20
    n_train_steps: 4000
    batch_size: 32
  policy:
    forward:
      type: mlp
      n_hid: 2048
      n_layers: 2
      checkpoint: fp
    backward:
      type: mlp
      shared_weights: True
  pct_offline: 0.0
  oracle:
    n: 500

regressor:
  weight_decay: 1e-4
  lr: 1e-4
  max_epochs: 100
  history: 15

dataset:
  fidelity:
    do: True 
    mixed: True
  split: random
  dataloader:
    train:
      batch_size: 256
    test:
      batch_size: 256
  path:
    oracle_dataset: null
      # train: null
      # test: null
# Number of objects to sample at the end of training
# Sample 5*K and choose topK
n_samples: 32
# Random seeds
seed: 0
# Device
device: cpu
# Float precision
float_precision: 32
al_n_rounds: 1

proxy:
  data_path: /home/mila/n/nikita.saxena/activelearning/storage/grid/nfid3_10x10.csv

multifidelity:
  # toy: True
  # n_fid: 3
  proxy: False
  candidate_set_path: /home/mila/n/nikita.saxena/activelearning/storage/grid/nfid3_10x10.csv
  # /home/mila/n/nikita.saxena/activelearning/storage/grid/grid3.csv
  # /home/mila/n/nikita.saxena/activelearning/storage/grid/nfid3_lower_quad.csv
  # /home/mila/n/nikita.saxena/activelearning/storage/grid/nfid3.csv
  rescale: 1
  fixed_cost: 1e-6

# Hydra config
hydra:
  # See: https://hydra.cc/docs/configure_hydra/workdir/
  run:
    dir: ${user.logdir.root}/${now:%Y-%m-%d_%H-%M-%S}
  job:
    # See: https://hydra.cc/docs/upgrades/1.1_to_1.2/changes_to_job_working_dir/
    # See: https://hydra.cc/docs/tutorials/basic/running_your_app/working_directory/#disable-changing-current-working-dir-to-jobs-output-dir
    chdir: True

defaults:
  # - _self_ # with _self_ the overrides don't seem to work...
  - dataset: ocp
  - oracle: ocp
  - env: crystal_surface
  - sampler: surface_gflownet # surface_gflownet # greedy # random # random_gflownet
  - selector: score # selector # score
  - surrogate: svdkl_kernel_wrapper
  - acquisition: botorch_mve #botorch_mve #botorch_ei #botorch_nei
  - user: default
  - logger: wandb
  # cime4r export significantly increases runtime --> only use when really needed; 
  # note: does not work when using gflownet yet, because gflownet produces new samples --> TODO
  - plotter: null # cime4r_ocp

logger:
  project_name: test_ocp_training
  run_name: ${sampler.id}_${surrogate.id}_samples-${n_samples}_${acquisition.id}_${seed}_max

dataset:
  # checkpoint_path: /network/scratch/a/alexandre.duval/ocp/runs/4648581/checkpoints/best_checkpoint.pt
  checkpoint_path: /network/scratch/a/alexandre.duval/ocp/catalyst-ckpts/0.0.1/best_checkpoint.pt
  data_path: /network/scratch/a/alexandre.duval/ocp/runs/4657270/deup_dataset
  # we want to split the training set into 10% (train) and 90% (validation) for testing purposes
  train_fraction: 0.1

surrogate:
  mll_args: 
    # number of training data instances
    num_data: 4059 #40593 
  feature_extractor:
    n_input: 352
    n_hidden: [265, 512, 265]
    n_output: 16
  train_epochs: 20
  lr: 0.01

device: cpu # cuda
float_precision: 32
budget: 10
n_samples: 100
seed: 98765
maximize: False


env:
  has_crystal_to_miller_constraints: True
  crystal_kwargs:
    do_sg_before_composition: True
    do_sg_to_lp_constraints: True
    do_sg_to_composition_constraints: True
    do_composition_to_sg_constraints: False
    composition_kwargs:
      elements: [1, 3, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 19, 23, 25, 26, 27, 28, 29, 34]
      max_diff_elem: 5
      min_diff_elem: 1
      min_atoms: 1
      max_atoms: 80
      min_atom_i: 1
      max_atom_i: 16
      do_charge_check: True
    space_group_kwargs:
      space_groups_subset: [16, 18, 19, 20, 21, 25, 26, 29, 30, 31, 33, 36, 38, 40, 41, 43, 44, 46, 47, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 74, 82, 84, 85, 86, 87, 88, 92, 99, 102, 107, 113, 114, 121, 122, 123, 126, 129, 131, 136, 137, 138, 139, 140, 141, 198, 199, 205, 206, 216, 217, 220, 221, 224, 225, 227, 229, 230]
    lattice_parameters_kwargs:
      min_length: 0.9
      max_length: 100.0
      min_angle: 50.0
      max_angle: 150.0
      n_comp: 5
      beta_params_min: 0.1
      beta_params_max: 100.0
      min_incr: 0.1
      fixed_distr_params:
        beta_weights: 1.0
        beta_alpha: 10.0
        beta_beta: 10.0
        bernoulli_eos_prob: 0.1
        bernoulli_bts_prob: 0.1
      random_distr_params:
        beta_weights: 1.0
        beta_alpha: 10.0
        beta_beta: 10.0
        bernoulli_eos_prob: 0.1
        bernoulli_bts_prob: 0.1
  miller_kwargs:
    is_hexagonal_rhombohedral: False
    max_increment: 1
    max_dim_per_action: 1
  reward_func: identity
  reward_beta: 1
  buffer:
    replay_capacity: 10
    train: null
    test: null

sampler: 
  conf:
    agent:
      random_action_prob: 0.1
      optimizer:
        batch_size:
          forward: 10
          backward_replay: 0
          backward_dataset: 0
        lr: 0.0001
        z_dim: 16
        lr_z_mult: 100
        n_train_steps: 100000
        lr_decay_period: 11000
        lr_decay_gamma: 0.5
      replay_sampling: weighted
      train_sampling: permutation
      evaluator:
        first_it: False
        period: -1
        checkpoints_period: 500
        n_trajs_logprobs: 100
        logprobs_batch_size: 10
        n: 10
        n_top_k: 5000
        top_k: 100
        top_k_period: -1

    policy:
      forward:
        type: mlp
        n_hid: 256
        n_layers: 3
        checkpoint: forward
      backward:
        type: mlp
        n_hid: 256
        n_layers: 3
        shared_weights: False
        checkpoint: backward
    proxy:
      reward_min: 1e-08
      do_clip_rewards: True
      # RBF exponential, to compute the proximity to a target
      reward_function: rbf_exponential
      # Parameters of the reward function
      reward_function_kwargs:
        beta: 8.0 # -8.0 # we want to maximize the acquisition values
        alpha: 1.0
        center: 0 # -1.6 # Energy of reaction
        distance: squared


# Hydra config
hydra:
  # See: https://hydra.cc/docs/configure_hydra/workdir/
  run:
    dir: ${user.logdir.root}/${now:%Y-%m-%d_%H-%M-%S}
  sweep:
    dir: ${user.logdir.root}/multirun/${now:%Y-%m-%d_%H-%M-%S}
  job:
    # See: https://hydra.cc/docs/upgrades/1.1_to_1.2/changes_to_job_working_dir/
    # See: https://hydra.cc/docs/tutorials/basic/running_your_app/working_directory/#disable-changing-current-working-dir-to-jobs-output-dir
    # need to have active learning directory for ocp config loading
    chdir: False #True

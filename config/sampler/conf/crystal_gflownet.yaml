defaults:
  - env: crystal_surface

# State flow modelling
state_flow: null

policy:
  forward:
    _target_: gflownet.policy.base.Policy
    config:
      type: mlp
      n_hid: 2048 #128
      n_layers: 2
      checkpoint: null
      reload_ckpt: False
      is_model: False

  backward: 
    _target_: gflownet.policy.base.Policy
    config: null

  shared: null


agent: 
  _target_: gflownet.gflownet.GFlowNetAgent
  # Random seeds
  seed: 0
  # Optimizer
  optimizer:
    z_dim: 16
    # Loss function
    loss: trajectorybalance
    # Learning rates
    lr: 0.001 # 0.0001 # <---
    lr_decay_period: 1000000
    lr_decay_gamma: 0.5
    lr_z_mult: 20 # 100
    method: adam
    # Threshold loss for early stopping
    early_stopping: 0.0
    # Coefficient for exponential moving average
    ema_alpha: 0.5
    # Optimizer: adam, sgd
    adam_beta1: 0.9
    adam_beta2: 0.999
    # Momentum for SGD
    sgd_momentum: 0.9
    # Number of trajectories of each kind
    batch_size:
      # Forward on-policy (possibly tempered and/or with random actions)
      forward: 16
      # Backward from training set
      backward_dataset: 0
      # Backward from replay buffer
      backward_replay: 0
    # Train to sample ratio
    train_to_sample_ratio: 1
    # Number of training iterations
    n_train_steps: 500 # 5000 # <---
    # From original implementation
    bootstrap_tau: 0.0
    clip_grad_norm: 0.0
  # If True, compute rewards in batches
  batch_reward: True
  # Force zero probability of sampling invalid actions
  mask_invalid_actions: True
  # Temperature for the logits /= temperature_logits
  temperature_logits: 1.0
  # Percentage of random actions e.g. 0.01
  random_action_prob: 0.0 # <---
  # Percentage of trajectories in a batch from an empirical distribution
  pct_offline: 0.0
  # Replay buffer
  replay_capacity: 0
  replay_sampling: permutation
  # Train data set backward sampling
  train_sampling: permutation
  num_empirical_loss: 200000
  oracle:
      # Number of samples for oracle metrics
      n: 50
  sample_only: False
  active_learning: False

  buffer:
    train: null
    test: null


logger:
  _target_: gflownet.utils.logger.Logger

  do:
    online: False # <---
    times: False

  project_name: "test_gflownet" # <---

  # Train metrics
  train:
    period: 1
  # Test metrics
  test:
    first_it: True
    period: 1000
    n: 100
    kde:
      bandwidth: 0.1
      kernel: gaussian
    n_top_k: 5000
    top_k: 100
    top_k_period: -1
    # Number of backward trajectories to estimate the log likelihood of each test data point
    n_trajs_logprobs: 10
    logprobs_batch_size: 100
    logprobs_bootstrap_size: 10000
    # Maximum number of test data points to compute log likelihood probs.
    max_data_logprobs: 1e5
  # Oracle metrics
  oracle:
    period: 100000
    k:
      - 1
      - 10
      - 100
  # Policy model checkpoints
  checkpoints:
    period: 1000

  # Log dir
  logdir:
    root: activelearning/gflownet/logs
    ckpts: activelearning/gflownetckpts
    overwrite: True
  debug: False
  lightweight: False
  progress: True
  context: "0"
  notes: null # wandb run notes (e.g. "baseline")
  tags: 
      - gflownet
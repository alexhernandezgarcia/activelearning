# @package _global_

defaults:
  # - _self_
  - env: amp
  - oracle@_oracle_dict.1: amp #branin1 #branin1
  # - oracle@_oracle_dict.2: hartmann2 #branin2
  # - oracle@_oracle_dict.3: hartmann3 #branin3
  - gflownet: random
  - user: nikita
  - logger: wandb
  - dataset: dataset
  - regressor: sf_dkl #mf_gp for branin, mf_svgp for hartmann
  - model: mlm_cnn 

regressor:
  surrogate:
    num_epochs: 512
    eval_period: 1
    patience: 15

logger:
  project_name: "AMP-DKL" #Scratch RandomAgent
  test:
    period: 1
  oracle:
    period: 1
  do:
    times: False
    online: True
  plot:
    period: -1
    first_it: True
  test:
    period: 100
    n: 1
  oracle:
    period: -1
  tags:
    - RANDOM

env:
  denorm_proxy: True #True for ucb, False for mes
  corr_type: from_trajectory
  proxy_state_format: state
  reward_func: boltzmann
  reward_beta: 2e1 #0.2 for ucb and 2e1 for mes
  # length: 10
  # reward_norm: 1e-3
  buffer:
    train:
      path: data_train.csv
      # /home/mila/n/nikita.saxena/activelearning/storage/amp/data_train.csv
      n: null
      type: all
      seed: null
      output_pkl: buffer_data_train.csv 
      output_csv: null
    test:
      path: /home/mila/n/nikita.saxena/activelearning/storage/dna/length30/test_2000_FINAL.csv #data_test.csv
      type: all
      output_pkl: buffer_data_test.csv
      n: null
      seed: null
      output_csv: null

dataset:
  normalize_data: True
# TODO can we get rid of "do"
  fidelity: 
    do: False
    mixed: False
  # n_samples: 30
  split: random #random
  train_fraction: 0.8
  dataloader:
    train:
      batch_size: 32
    test:
      batch_size: 32
  path:
    type: sf
    oracle_dataset:
      train: 
        path: /home/mila/n/nikita.saxena/activelearning/storage/amp/sf/data_train.csv
        get_scores: False
      test: 
        path: /home/mila/n/nikita.saxena/activelearning/storage/amp/sf/data_test.csv
        get_scores: False
      # train: 
        # path: /home/mila/n/nikita.saxena/activelearning/storage/branin/nfid1_100x100.csv
        # get_scores: True
      # test: null

# Number of objects to sample at the end of training
# Sample 5*K and choose topK
n_samples: 128
# Random seeds
seed: 0
# Device
device: cuda
# Float precision
float_precision: 32
al_n_rounds: 30
do_figure: False

multifidelity:
  toy: True
  rescale: 1
  proxy: True
  # candidate_set_path: /home/mila/n/nikita.saxena/activelearning/storage/branin/nfid3_100x100.csv

# Hydra config
hydra:
  # See: https://hydra.cc/docs/configure_hydra/workdir/
  run:
    dir: ${user.logdir.root}/${now:%Y-%m-%d_%H-%M-%S}
  job:
    # See: https://hydra.cc/docs/upgrades/1.1_to_1.2/changes_to_job_working_dir/
    # See: https://hydra.cc/docs/tutorials/basic/running_your_app/working_directory/#disable-changing-current-working-dir-to-jobs-output-dir
    chdir: True